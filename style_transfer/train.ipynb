{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import Generator, load_rgb_img\n",
    "from ImageTransformationNN import ImageTransformationNN\n",
    "from VGG16 import VGG16LossNN\n",
    "\n",
    "import time, argparse\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to utils.py\n",
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    # normalize using imagenet mean and std\n",
    "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    batch = batch.div_(255.0)\n",
    "    res = (batch - mean) / std\n",
    "    print('r', res.size())\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALI_DATA_DIR=r\"/Users/aliissaoui/Desktop/studies/IIT/spring/CS577/Project/PerceptualLossNetwork/dataset/train_images/\"\n",
    "ALI_STYLE_DIR=r\"/Users/aliissaoui/Desktop/studies/IIT/spring/CS577/Project/PerceptualLossNetwork/dataset/train_styles\"\n",
    "ALI_STYLE_PATH = f\"{ALI_STYLE_DIR}/mosaic.jpeg\"\n",
    "BATCH_SIZE = 4\n",
    "DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"parser = argparse.ArgumentParser(description='Arguments for the network.')\\n\\nparser.add_argument('learning_rate', type=float,\\n                    help='Learning rate')\\n\\nparser.add_argument('epochs', type=int,\\n                    help='Number of epochs')\\n\\nparser.add_argument('content_weights', type=int,\\n                    help='Content weights')\\n\\nparser.add_argument('log_interval', type=int,\\n                    help='Integer for Log interval')\\n\\nparser.add_argument('checkpoint_model_dir', type=str,\\n                    help='Path for checkpoints directory')  \\n\\nparser.add_argument('checkpoint_interval', type=int,\\n                    help='Integer for checkpoints interval') \\n\\nargs = parser.parse_args()\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parser: Temporarily disabled for jupyter notebook\n",
    "\"\"\"parser = argparse.ArgumentParser(description='Arguments for the network.')\n",
    "\n",
    "parser.add_argument('learning_rate', type=float,\n",
    "                    help='Learning rate')\n",
    "\n",
    "parser.add_argument('epochs', type=int,\n",
    "                    help='Number of epochs')\n",
    "\n",
    "parser.add_argument('content_weights', type=int,\n",
    "                    help='Content weights')\n",
    "\n",
    "parser.add_argument('log_interval', type=int,\n",
    "                    help='Integer for Log interval')\n",
    "\n",
    "parser.add_argument('checkpoint_model_dir', type=str,\n",
    "                    help='Path for checkpoints directory')  \n",
    "\n",
    "parser.add_argument('checkpoint_interval', type=int,\n",
    "                    help='Integer for checkpoints interval') \n",
    "\n",
    "args = parser.parse_args()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferFactory(object):\n",
    "    \n",
    "    def __init__(self, DATA_DIR, STYLE_PATH, BATCH_SIZE):\n",
    "        self.loss_net = VGG16LossNN()\n",
    "        self.transformer = ImageTransformationNN()\n",
    "        self.gen = Generator(DATA_DIR, BATCH_SIZE)\n",
    "        self.style = load_rgb_img(STYLE_PATH)\n",
    "        #print(\"shape:\", self.style.shape)\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        '''\n",
    "        Should train network and\n",
    "        - save model file\n",
    "        - save training info, validaiton loss, etc\n",
    "        '''\n",
    "\n",
    "        \n",
    "        # Parameters ( to be placed in parser )\n",
    "        learning_rate = 1e-3 \n",
    "        epochs = 2\n",
    "        content_weights = 1e5\n",
    "        style_weights = 1e10\n",
    "        log_interval = 1\n",
    "        checkpoint_model_dir = None\n",
    "        checkpoint_interval = 0\n",
    "        #! \n",
    "        \n",
    "        optimizer = Adam(self.transformer.parameters(), learning_rate)\n",
    "\n",
    "        mse_loss = MSELoss()\n",
    "\n",
    "        style_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])\n",
    "        \n",
    "        style = style_transform(self.style)\n",
    "        style = style.repeat(BATCH_SIZE, 1, 1, 1)\n",
    "        \n",
    "        loss_net_input = normalize_batch(style)\n",
    "        \n",
    "        feature_style = self.loss_net.forward(normalize_batch(style))\n",
    "        \n",
    "        \"\"\"Verification\"\"\"\n",
    "        for key, value in feature_style.items() :\n",
    "            print(key)\n",
    "        \n",
    "        gram_style = [gram_matrix(y) for y in feature_style.values()]\n",
    "        print('GRAM:', gram_style[0].shape)\n",
    "    \n",
    "              \n",
    "        for epoch in range(epochs):\n",
    "            print('epoch:', epoch)\n",
    "            self.transformer.train()\n",
    "            print('-------------\\n', self.transformer, '\\n-------------')\n",
    "\n",
    "            l_feat_total = 0.\n",
    "            l_style_total = 0.\n",
    "            count = 0\n",
    "            \n",
    "            # To revise\n",
    "            for batch_id, (x, _) in enumerate(self.gen):\n",
    "                \n",
    "                # Add the batch size\n",
    "                n_batch = len(x)\n",
    "                count += n_batch\n",
    "                \n",
    "                # Adam\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Problem here !!! \n",
    "                y = self.transformer(x)\n",
    "                input()\n",
    "                \n",
    "                # Normalize batch\n",
    "                y = normalize_batch(y)\n",
    "                x = normalize_batch(x)\n",
    "                \n",
    "                # Features from the VGG16 network\n",
    "                features_y = self.loss_net(y)\n",
    "                features_x = self.loss_net(x)\n",
    "                \n",
    "                # Update features reconstruction loss\n",
    "                l_feat = content_weights * mse_loss(features_y['relu2_2'], features_x['relu2_2'])\n",
    "                \n",
    "                \n",
    "                l_style = 0.\n",
    "                \n",
    "                for f_y, gram_s in zip(features_y, gram_style):\n",
    "                    gram_y = gram_matrix(f_y)\n",
    "                    l_style += mse_loss(gram_y, gram_s[:n_batch, :, :])\n",
    "                    \n",
    "                l_style *= style_weights\n",
    "                \n",
    "                l_total = l_feat + l_style\n",
    "                \n",
    "                l_total.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                l_feat_total += l_feat.item()\n",
    "                l_style_total += l_style.item()\n",
    "                \n",
    "                if (batch_id + 1) % log_interval == 0:\n",
    "                    msg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "                        time.ctime(), epoch + 1, count, len(train_dataset),\n",
    "                                      l_feat_total / (batch_id + 1),\n",
    "                                      l_style_total / (batch_id + 1),\n",
    "                                      (l_feat_total + l_style_total) / (batch_id + 1)\n",
    "                    )\n",
    "                    print(msg)\n",
    "                    \n",
    "                if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
    "                    transformer.eval().cpu()\n",
    "                    filename = \"check_epoch_\" + str(epoch) + \"batch_id\" + str(batch_id + 1) + '.pth'\n",
    "                    path = os.path.join(checkpoint_model_dir, filename)\n",
    "                    torch.save(transformer.state_dict(), path)\n",
    "                    #transformer.to(device).train() ?\n",
    "                    \n",
    "        transformer.eval().cpu()\n",
    "        \n",
    "        if checkpoint_model_dir is not None:\n",
    "            filename = \"check_epoch_\" + str(epoch) + \"batch_id\" + str(batch_id + 1) + '.pth'\n",
    "            path = os.path.join(checkpoint_model_dir, filename)\n",
    "            torch.save(transformer.state_dict(), path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''  \n",
    "    Parse args, such as data dir, style image, etc.\n",
    "    Call factory object and train\n",
    "    '''\n",
    "    \n",
    "    #parser = argparse.ArgumentParser(description='Arguments for the training.')\n",
    "\n",
    "    model = StyleTransferFactory(ALI_DATA_DIR, ALI_STYLE_PATH, BATCH_SIZE)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r torch.Size([4, 3, 391, 470])\n",
      "r torch.Size([4, 3, 391, 470])\n",
      "relu1_2\n",
      "relu2_2\n",
      "relu3_3\n",
      "relu4_3\n",
      "GRAM: torch.Size([4, 64, 64])\n",
      "epoch: 0\n",
      "-------------\n",
      " ImageTransformationNN(\n",
      "  (down_sample): DownSampleConv(\n",
      "    (conv2d1): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu1): ReLU()\n",
      "    (conv2d2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu2): ReLU()\n",
      "    (conv2d3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (norm3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu3): ReLU()\n",
      "  )\n",
      "  (res): ResidualNet(\n",
      "    (block1): RBlock(\n",
      "      (conv2d1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block2): RBlock(\n",
      "      (conv2d1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block3): RBlock(\n",
      "      (conv2d1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block4): RBlock(\n",
      "      (conv2d1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block5): RBlock(\n",
      "      (conv2d1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (up_sample): UpSampleConv(\n",
      "    (conv2d1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(0.5, 0.5))\n",
      "    (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu1): ReLU()\n",
      "    (conv2d2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(0.5, 0.5))\n",
      "    (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu2): ReLU()\n",
      "    (conv2d3): ConvTranspose2d(32, 3, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (norm3): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      ") \n",
      "-------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 3, 3, 3], expected input[4, 128, 59, 59] to have 3 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8d16891223ff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleTransferFactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALI_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALI_STYLE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-acc4e26c26b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# Problem here !!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/Desktop/studies/IIT/spring/CS577/Project/PerceptualLossNetwork/style_transfer/ImageTransformationNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 )))\"\"\"\n\u001b[1;32m     21\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/Desktop/studies/IIT/spring/CS577/Project/PerceptualLossNetwork/style_transfer/ImageTransformationNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    102\u001b[0m             self.block2(\n\u001b[1;32m    103\u001b[0m             self.block1(\n\u001b[0;32m--> 104\u001b[0;31m             X)))))\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/Desktop/studies/IIT/spring/CS577/Project/PerceptualLossNetwork/style_transfer/ImageTransformationNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     78\u001b[0m                     self.norm1(\n\u001b[1;32m     79\u001b[0m                     self.conv2d1(\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     )))\n\u001b[1;32m     82\u001b[0m                 ))\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 3, 3, 3], expected input[4, 128, 59, 59] to have 3 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
