{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import Generator, load_rgb_img\n",
    "from ImageTransformationNN import ImageTransformationNN\n",
    "from VGG16 import VGG16LossNN\n",
    "\n",
    "import time, argparse\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to utils.py\n",
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    # normalize using imagenet mean and std\n",
    "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    batch = batch.div_(255.0)\n",
    "    res = (batch - mean) / std\n",
    "    #print('r', res.size())\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALI_DATA_DIR=r\"/Users/aliissaoui/Desktop/studies/IIT/spring/CS577/Project/PerceptualLossNetwork/dataset/train_images/\"\n",
    "ALI_STYLE_DIR=r\"/Users/aliissaoui/Desktop/studies/IIT/spring/CS577/Project/PerceptualLossNetwork/dataset/train_styles\"\n",
    "ALI_STYLE_PATH = f\"{ALI_STYLE_DIR}/mosaic.jpeg\"\n",
    "BATCH_SIZE = 4\n",
    "DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"parser = argparse.ArgumentParser(description='Arguments for the network.')\\n\\nparser.add_argument('learning_rate', type=float,\\n                    help='Learning rate')\\n\\nparser.add_argument('epochs', type=int,\\n                    help='Number of epochs')\\n\\nparser.add_argument('content_weights', type=int,\\n                    help='Content weights')\\n\\nparser.add_argument('log_interval', type=int,\\n                    help='Integer for Log interval')\\n\\nparser.add_argument('checkpoint_model_dir', type=str,\\n                    help='Path for checkpoints directory')  \\n\\nparser.add_argument('checkpoint_interval', type=int,\\n                    help='Integer for checkpoints interval') \\n\\nargs = parser.parse_args()\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parser: Temporarily disabled for jupyter notebook\n",
    "\"\"\"parser = argparse.ArgumentParser(description='Arguments for the network.')\n",
    "\n",
    "parser.add_argument('learning_rate', type=float,\n",
    "                    help='Learning rate')\n",
    "\n",
    "parser.add_argument('epochs', type=int,\n",
    "                    help='Number of epochs')\n",
    "\n",
    "parser.add_argument('content_weights', type=int,\n",
    "                    help='Content weights')\n",
    "\n",
    "parser.add_argument('log_interval', type=int,\n",
    "                    help='Integer for Log interval')\n",
    "\n",
    "parser.add_argument('checkpoint_model_dir', type=str,\n",
    "                    help='Path for checkpoints directory')  \n",
    "\n",
    "parser.add_argument('checkpoint_interval', type=int,\n",
    "                    help='Integer for checkpoints interval') \n",
    "\n",
    "args = parser.parse_args()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferFactory(object):\n",
    "    \n",
    "    def __init__(self, DATA_DIR, STYLE_PATH, BATCH_SIZE):\n",
    "        self.loss_net = VGG16LossNN()\n",
    "        self.transformer = ImageTransformationNN()\n",
    "        self.gen = Generator(DATA_DIR, BATCH_SIZE)\n",
    "        self.style = load_rgb_img(STYLE_PATH)\n",
    "        #print(\"shape:\", self.style.shape)\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        '''\n",
    "        Should train network and\n",
    "        - save model file\n",
    "        - save training info, validaiton loss, etc\n",
    "        '''\n",
    "\n",
    "        \n",
    "        # Parameters ( to be placed in parser )\n",
    "        learning_rate = 1e-3 \n",
    "        epochs = 2\n",
    "        content_weights = 1e5\n",
    "        style_weights = 1e10\n",
    "        log_interval = 1\n",
    "        checkpoint_model_dir = None\n",
    "        checkpoint_interval = 0\n",
    "        #! \n",
    "        \n",
    "        optimizer = Adam(self.transformer.parameters(), learning_rate)\n",
    "\n",
    "        mse_loss = MSELoss()\n",
    "\n",
    "        style_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])\n",
    "        \n",
    "        style = style_transform(self.style)\n",
    "        style = style.repeat(BATCH_SIZE, 1, 1, 1)\n",
    "        #print('style:', style.shape)\n",
    "        \n",
    "        loss_net_input = normalize_batch(style)\n",
    "        \n",
    "        print('style:', style.shape)\n",
    "        feature_style = self.loss_net.forward(normalize_batch(style))\n",
    "        \n",
    "        \"\"\"Verification\"\"\"\n",
    "        #for key, value in feature_style.items() :\n",
    "        #    print(key)\n",
    "        \n",
    "        gram_style = [gram_matrix(y) for y in feature_style.values()]\n",
    "        #print('GRAM:', gram_style[0].shape)\n",
    "    \n",
    "              \n",
    "        for epoch in range(epochs):\n",
    "            print('epoch:', epoch)\n",
    "            self.transformer.train()\n",
    "            print('-------------\\n', self.transformer, '\\n-------------')\n",
    "\n",
    "            l_feat_total = 0.\n",
    "            l_style_total = 0.\n",
    "            count = 0\n",
    "            \n",
    "            # To revise\n",
    "            for batch_id, (x, _) in enumerate(self.gen):\n",
    "                \n",
    "                # Add the batch size\n",
    "                n_batch = len(x)\n",
    "                count += n_batch\n",
    "                \n",
    "                # Adam\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #print(\"XXXX\", x.shape)\n",
    "                # Problem here !!! \n",
    "                y = self.transformer(x)\n",
    "                \n",
    "                # Normalize batch\n",
    "                y = normalize_batch(y)\n",
    "                x = normalize_batch(x)\n",
    "                \n",
    "                # Features from the VGG16 network\n",
    "                print('x size:', x.shape)\n",
    "                print('y size:', y.shape)\n",
    "                \n",
    "                features_y = self.loss_net(y)\n",
    "                features_x = self.loss_net(x)\n",
    "                \n",
    "                # Update features reconstruction loss\n",
    "                print(\"feature y \", features_y['relu2_2'].shape)\n",
    "                print(\"feature x \", features_x['relu2_2'].shape)\n",
    "                mse = mse_loss(features_y['relu2_2'], features_x['relu2_2'])\n",
    "                \n",
    "                print('mse', mse.shape)\n",
    "                l_feat = content_weights * mse\n",
    "                \n",
    "                \n",
    "                l_style = 0.\n",
    "                \n",
    "                for f_y, gram_s in zip(features_y, gram_style):\n",
    "                    gram_y = gram_matrix(f_y)\n",
    "                    l_style += mse_loss(gram_y, gram_s[:n_batch, :, :])\n",
    "                    \n",
    "                l_style *= style_weights\n",
    "                \n",
    "                l_total = l_feat + l_style\n",
    "                \n",
    "                l_total.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                l_feat_total += l_feat.item()\n",
    "                l_style_total += l_style.item()\n",
    "                \n",
    "                if (batch_id + 1) % log_interval == 0:\n",
    "                    msg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
    "                        time.ctime(), epoch + 1, count, len(train_dataset),\n",
    "                                      l_feat_total / (batch_id + 1),\n",
    "                                      l_style_total / (batch_id + 1),\n",
    "                                      (l_feat_total + l_style_total) / (batch_id + 1)\n",
    "                    )\n",
    "                    print(msg)\n",
    "                    \n",
    "                if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
    "                    transformer.eval().cpu()\n",
    "                    filename = \"check_epoch_\" + str(epoch) + \"batch_id\" + str(batch_id + 1) + '.pth'\n",
    "                    path = os.path.join(checkpoint_model_dir, filename)\n",
    "                    torch.save(transformer.state_dict(), path)\n",
    "                    #transformer.to(device).train() ?\n",
    "                    \n",
    "        transformer.eval().cpu()\n",
    "        \n",
    "        if checkpoint_model_dir is not None:\n",
    "            filename = \"check_epoch_\" + str(epoch) + \"batch_id\" + str(batch_id + 1) + '.pth'\n",
    "            path = os.path.join(checkpoint_model_dir, filename)\n",
    "            torch.save(transformer.state_dict(), path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''  \n",
    "    Parse args, such as data dir, style image, etc.\n",
    "    Call factory object and train\n",
    "    '''\n",
    "    \n",
    "    #parser = argparse.ArgumentParser(description='Arguments for the training.')\n",
    "\n",
    "    model = StyleTransferFactory(ALI_DATA_DIR, ALI_STYLE_PATH, BATCH_SIZE)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: torch.Size([4, 3, 391, 470])\n",
      "epoch: 0\n",
      "-------------\n",
      " ImageTransformationNN(\n",
      "  (down_sample): DownSampleConv(\n",
      "    (conv2d1): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (norm1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu1): ReLU()\n",
      "    (conv2d2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu2): ReLU()\n",
      "    (conv2d3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (norm3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu3): ReLU()\n",
      "  )\n",
      "  (res): ResidualNet(\n",
      "    (block1): RBlock(\n",
      "      (conv2d1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block2): RBlock(\n",
      "      (conv2d1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block3): RBlock(\n",
      "      (conv2d1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block4): RBlock(\n",
      "      (conv2d1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (block5): RBlock(\n",
      "      (conv2d1): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2d2): ConvLayer(\n",
      "        (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (up_sample): UpSampleConv(\n",
      "    (conv2d1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu1): ReLU()\n",
      "    (conv2d2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (norm2): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (relu2): ReLU()\n",
      "    (conv2d3): ConvTranspose2d(32, 3, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (norm3): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      ") \n",
      "-------------\n",
      "block 4 done\n",
      "conv2d1 done torch.Size([4, 64, 61, 61])\n",
      "norm1 done torch.Size([4, 64, 61, 61])\n",
      "relu done torch.Size([4, 64, 61, 61])\n",
      "conv2d2 done torch.Size([4, 32, 63, 63])\n",
      "norm2 done torch.Size([4, 32, 63, 63])\n",
      "relu2 done torch.Size([4, 32, 63, 63])\n",
      "conv2d3 done torch.Size([4, 3, 71, 71])\n",
      "norm3 done torch.Size([4, 3, 71, 71])\n",
      "tanh done torch.Size([4, 3, 71, 71])\n",
      "x size: torch.Size([4, 3, 256, 256])\n",
      "y size: torch.Size([4, 3, 71, 71])\n",
      "feature y  torch.Size([4, 128, 35, 35])\n",
      "feature x  torch.Size([4, 128, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([4, 128, 128, 128])) that is different to the input size (torch.Size([4, 128, 35, 35])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (35) must match the size of tensor b (128) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-8d16891223ff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleTransferFactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALI_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALI_STYLE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-db13076d2039>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature y \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu2_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature x \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu2_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu2_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu2_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2540\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2543\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliissaoui/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (35) must match the size of tensor b (128) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_net = VGG16LossNN()\n",
    "transformer = ImageTransformationNN()\n",
    "gen = Generator(ALI_DATA_DIR, BATCH_SIZE)\n",
    "style = load_rgb_img(ALI_STYLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(loss_net, (3, 391, 470))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(transformer, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters ( to be placed in parser )\n",
    "learning_rate = 1e-3 \n",
    "epochs = 2\n",
    "content_weights = 1e5\n",
    "style_weights = 1e10\n",
    "log_interval = 1\n",
    "checkpoint_model_dir = None\n",
    "checkpoint_interval = 0\n",
    "#! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(self.transformer.parameters(), learning_rate)\n",
    "\n",
    "mse_loss = MSELoss()\n",
    "\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "style = style_transform(self.style)\n",
    "style = style.repeat(BATCH_SIZE, 1, 1, 1)\n",
    "\n",
    "loss_net_input = normalize_batch(style)\n",
    "\n",
    "feature_style = self.loss_net.forward(normalize_batch(style))\n",
    "\n",
    "\"\"\"Verification\"\"\"\n",
    "for key, value in feature_style.items() :\n",
    "    print(key)\n",
    "\n",
    "gram_style = [gram_matrix(y) for y in feature_style.values()]\n",
    "print('GRAM:', gram_style[0].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
